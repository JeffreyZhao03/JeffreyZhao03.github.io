[
  {
    "objectID": "posts/Problem Set 3/index.html#step-2",
    "href": "posts/Problem Set 3/index.html#step-2",
    "title": "Problem Set 3",
    "section": "Step 2",
    "text": "Step 2\nThis document has a total of 1,436 observations and 39 columns with different features. There are 36 numeric data types. There are 3 categorical data types.\n\ncars = read_csv(\"ToyotaCorolla.csv\")\n\nRows: 1436 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Model, Fuel_Type, Color\ndbl (36): Id, Price, Age_08_04, Mfg_Month, Mfg_Year, KM, HP, Met_Color, Auto...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(cars)\n\nRows: 1,436\nColumns: 39\n$ Id                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ Model             &lt;chr&gt; \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\", \"TO…\n$ Price             &lt;dbl&gt; 13500, 13750, 13950, 14950, 13750, 12950, 16900, 186…\n$ Age_08_04         &lt;dbl&gt; 23, 23, 24, 26, 30, 32, 27, 30, 27, 23, 25, 22, 25, …\n$ Mfg_Month         &lt;dbl&gt; 10, 10, 9, 7, 3, 1, 6, 3, 6, 10, 8, 11, 8, 2, 1, 5, …\n$ Mfg_Year          &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002…\n$ KM                &lt;dbl&gt; 46986, 72937, 41711, 48000, 38500, 61000, 94612, 758…\n$ Fuel_Type         &lt;chr&gt; \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"D…\n$ HP                &lt;dbl&gt; 90, 90, 90, 90, 90, 90, 90, 90, 192, 69, 192, 192, 1…\n$ Met_Color         &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1…\n$ Color             &lt;chr&gt; \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"White\",…\n$ Automatic         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ CC                &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 1800…\n$ Doors             &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ Cylinders         &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4…\n$ Gears             &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5…\n$ Quarterly_Tax     &lt;dbl&gt; 210, 210, 210, 210, 210, 210, 210, 210, 100, 185, 10…\n$ Weight            &lt;dbl&gt; 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245, 1185…\n$ Mfr_Guarantee     &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0…\n$ BOVAG_Guarantee   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Guarantee_Period  &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 3, 3, 3, 3, 3, 3, …\n$ ABS               &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_1          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_2          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Airco             &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Automatic_airco   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Boardcomputer     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ CD_Player         &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0…\n$ Central_Lock      &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Powered_Windows   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Power_Steering    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Radio             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Mistlamps         &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Sport_Model       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0…\n$ Backseat_Divider  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Metallic_Rim      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Radio_cassette    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Parking_Assistant &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Tow_Bar           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n\n\n\ncars = cars %&gt;%\n  select (-Id, -Model, -Mfg_Month, -Cylinders) %&gt;%  \n  rename(Age = Age_08_04)\n\nIt would benefit us if our features were better represented. For example, we can change some of the features to nominal data types. After changing them into that, we can change them into factor data. This helps us determine if there are any missing values in the data set.\n\ncars_fct =cars %&gt;% \n  select(-Price, -Age, -KM, -HP, -CC, -Weight, -Quarterly_Tax) %&gt;%\n  mutate_all(.funs = factor)\n  \ncars_num = cars %&gt;% \n  select(Price, Age, KM, HP, CC, Weight, Quarterly_Tax)\n\n\ncars= bind_cols(cars_num, cars_fct)  \n\nThere are no missing values for each feature, so we don’t have to put values in.\n\nsummary(cars_num)\n\n     Price            Age              KM               HP       \n Min.   : 4350   Min.   : 1.00   Min.   :     1   Min.   : 69.0  \n 1st Qu.: 8450   1st Qu.:44.00   1st Qu.: 43000   1st Qu.: 90.0  \n Median : 9900   Median :61.00   Median : 63390   Median :110.0  \n Mean   :10731   Mean   :55.95   Mean   : 68533   Mean   :101.5  \n 3rd Qu.:11950   3rd Qu.:70.00   3rd Qu.: 87021   3rd Qu.:110.0  \n Max.   :32500   Max.   :80.00   Max.   :243000   Max.   :192.0  \n       CC            Weight     Quarterly_Tax   \n Min.   : 1300   Min.   :1000   Min.   : 19.00  \n 1st Qu.: 1400   1st Qu.:1040   1st Qu.: 69.00  \n Median : 1600   Median :1070   Median : 85.00  \n Mean   : 1577   Mean   :1072   Mean   : 87.12  \n 3rd Qu.: 1600   3rd Qu.:1085   3rd Qu.: 85.00  \n Max.   :16000   Max.   :1615   Max.   :283.00"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#step-3",
    "href": "posts/Problem Set 3/index.html#step-3",
    "title": "Problem Set 3",
    "section": "Step 3",
    "text": "Step 3\n\nlm_Price=train(Price ~ .,\n               data =cars, \n               method = \"lm\")\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nWarning in predict.lm(modelFit, newdata): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nlm_Price\n\nLinear Regression \n\n1436 samples\n  34 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 1436, 1436, 1436, 1436, 1436, 1436, ... \nResampling results:\n\n  RMSE     Rsquared   MAE     \n  1378.17  0.8500533  850.9737\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\nsummary(lm_Price)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5225.2  -620.6   -41.6   575.3  6192.5 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -2.234e+03  1.796e+03  -1.244 0.213732    \nAge                -2.902e+01  8.801e+00  -3.297 0.001003 ** \nKM                 -1.589e-02  1.085e-03 -14.642  &lt; 2e-16 ***\nHP                  2.230e+01  3.116e+00   7.158 1.33e-12 ***\nCC                 -6.027e-02  7.374e-02  -0.817 0.413890    \nWeight              7.533e+00  1.166e+00   6.460 1.45e-10 ***\nQuarterly_Tax       1.122e+01  1.655e+00   6.779 1.78e-11 ***\nMfg_Year1999        7.023e+02  1.320e+02   5.322 1.20e-07 ***\nMfg_Year2000        1.541e+03  2.315e+02   6.659 3.98e-11 ***\nMfg_Year2001        2.313e+03  3.284e+02   7.044 2.95e-12 ***\nMfg_Year2002        4.489e+03  4.482e+02  10.018  &lt; 2e-16 ***\nMfg_Year2003        6.025e+03  5.338e+02  11.286  &lt; 2e-16 ***\nMfg_Year2004        7.967e+03  6.491e+02  12.274  &lt; 2e-16 ***\nFuel_TypeDiesel     1.002e+03  3.098e+02   3.233 0.001255 ** \nFuel_TypePetrol     1.502e+03  3.242e+02   4.635 3.91e-06 ***\nMet_Color1         -6.757e+01  6.820e+01  -0.991 0.322000    \nColorBlack          6.014e+02  6.166e+02   0.975 0.329563    \nColorBlue           4.780e+02  6.158e+02   0.776 0.437672    \nColorGreen          3.340e+02  6.169e+02   0.541 0.588304    \nColorGrey           6.352e+02  6.160e+02   1.031 0.302672    \nColorRed            4.733e+02  6.159e+02   0.768 0.442344    \nColorSilver         5.582e+02  6.211e+02   0.899 0.368984    \nColorViolet         2.224e+02  8.171e+02   0.272 0.785563    \nColorWhite         -2.128e+02  6.450e+02  -0.330 0.741514    \nColorYellow         5.609e+02  8.670e+02   0.647 0.517744    \nAutomatic1          4.620e+02  1.325e+02   3.488 0.000503 ***\nDoors3             -4.380e+02  7.626e+02  -0.574 0.565842    \nDoors4             -4.341e+02  7.679e+02  -0.565 0.571948    \nDoors5             -2.798e+02  7.641e+02  -0.366 0.714255    \nGears4             -1.128e+02  1.328e+03  -0.085 0.932337    \nGears5              5.834e+02  7.879e+02   0.741 0.459109    \nGears6              1.011e+03  8.101e+02   1.247 0.212449    \nMfr_Guarantee1      2.968e+02  6.310e+01   4.704 2.81e-06 ***\nBOVAG_Guarantee1    4.583e+02  1.102e+02   4.158 3.40e-05 ***\nGuarantee_Period6   5.197e+02  1.669e+02   3.114 0.001884 ** \nGuarantee_Period12  7.091e+02  1.671e+02   4.244 2.34e-05 ***\nGuarantee_Period13  3.974e+03  1.097e+03   3.622 0.000303 ***\nGuarantee_Period18  2.862e+03  1.078e+03   2.655 0.008023 ** \nGuarantee_Period20  1.387e+03  1.079e+03   1.285 0.198945    \nGuarantee_Period24  4.352e+02  5.627e+02   0.773 0.439421    \nGuarantee_Period28  1.668e+03  1.080e+03   1.545 0.122632    \nGuarantee_Period36  1.138e+02  5.743e+02   0.198 0.842910    \nABS1               -4.491e+01  1.126e+02  -0.399 0.689972    \nAirbag_11           1.535e+02  2.156e+02   0.712 0.476656    \nAirbag_21          -1.855e+00  1.189e+02  -0.016 0.987554    \nAirco1              2.356e+02  7.626e+01   3.089 0.002047 ** \nAutomatic_airco1    1.918e+03  1.683e+02  11.395  &lt; 2e-16 ***\nBoardcomputer1     -1.630e+02  1.092e+02  -1.493 0.135603    \nCD_Player1          2.170e+02  8.486e+01   2.557 0.010676 *  \nCentral_Lock1      -9.588e+01  1.210e+02  -0.792 0.428242    \nPowered_Windows1    3.032e+02  1.214e+02   2.497 0.012649 *  \nPower_Steering1    -1.597e+02  2.421e+02  -0.660 0.509435    \nRadio1              5.816e+02  6.244e+02   0.931 0.351791    \nMistlamps1          2.806e+01  9.299e+01   0.302 0.762902    \nSport_Model1       -3.963e+01  8.145e+01  -0.487 0.626634    \nBackseat_Divider1  -4.920e+01  1.308e+02  -0.376 0.706793    \nMetallic_Rim1       1.146e+02  8.199e+01   1.398 0.162472    \nRadio_cassette1    -6.200e+02  6.239e+02  -0.994 0.320488    \nParking_Assistant1 -4.225e+02  5.400e+02  -0.782 0.434109    \nTow_Bar1           -1.724e+02  6.709e+01  -2.570 0.010263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1049 on 1376 degrees of freedom\nMultiple R-squared:  0.9197,    Adjusted R-squared:  0.9163 \nF-statistic: 267.2 on 59 and 1376 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#step-4",
    "href": "posts/Problem Set 3/index.html#step-4",
    "title": "Problem Set 3",
    "section": "Step 4",
    "text": "Step 4\n\ncaret::featurePlot(keep(cars, is.numeric), cars$Price, plot = \"scatter\")"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#step-5",
    "href": "posts/Problem Set 3/index.html#step-5",
    "title": "Problem Set 3",
    "section": "Step 5",
    "text": "Step 5"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#step-6",
    "href": "posts/Problem Set 3/index.html#step-6",
    "title": "Problem Set 3",
    "section": "Step 6",
    "text": "Step 6\n\ncars_dum = dummy(cars, int = TRUE)\ncars_num = cars %&gt;% \n  keep(is.numeric)\ncars = bind_cols(cars_num, cars_dum)\nrm(cars_dum, cars_num)\n\n\nset.seed(4532)\nsamp = createDataPartition(cars$Price, p=0.7, list =FALSE)\ntraining = cars[samp, ]\ntesting = cars [-samp, ]\nrm(samp)"
  },
  {
    "objectID": "posts/Problem Set 3/index.html#step-7",
    "href": "posts/Problem Set 3/index.html#step-7",
    "title": "Problem Set 3",
    "section": "Step 7",
    "text": "Step 7\n\ntrain_ctrl =trainControl(method = \"repeatedcv\", number =20, repeats =10)\ntree =train(Price ~ .,\n            data= training, \n            method= \"rpart\",\n            trControl = train_ctrl,\n            tuneGrid= expand.grid(cp = seq(0.0, 0.1, 0.01)))\n            control =rpart.control(method = \"anova\", minsplit =1, minibucket =1)\ntree\n\nCART \n\n1007 samples\n  87 predictor\n\nNo pre-processing\nResampling: Cross-Validated (20 fold, repeated 10 times) \nSummary of sample sizes: 956, 956, 955, 957, 956, 958, ... \nResampling results across tuning parameters:\n\n  cp    RMSE      Rsquared   MAE      \n  0.00  1219.175  0.8873675   892.6483\n  0.01  1370.648  0.8564458  1014.9461\n  0.02  1610.301  0.8015575  1213.7059\n  0.03  1626.717  0.7978216  1228.9618\n  0.04  1710.805  0.7810633  1250.6468\n  0.05  1729.339  0.7765759  1263.5434\n  0.06  1729.339  0.7765759  1263.5434\n  0.07  1729.339  0.7765759  1263.5434\n  0.08  1729.339  0.7765759  1263.5434\n  0.09  1729.339  0.7765759  1263.5434\n  0.10  1729.339  0.7765759  1263.5434\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0."
  },
  {
    "objectID": "posts/Problem Set 3/index.html#step-8",
    "href": "posts/Problem Set 3/index.html#step-8",
    "title": "Problem Set 3",
    "section": "Step 8",
    "text": "Step 8\n\nlm_model &lt;- lm(Price ~ ., data = cars)\n\n# Calculate feature importance\nvip::vip(lm_model)\n\n\n\n\nThe features with the most importance would be KM, Mfg_Year_2001, and Automatic.The featurs that may be ok to remove would be quarterly tax and HP."
  },
  {
    "objectID": "posts/Problem Set 3/index.html#step-9",
    "href": "posts/Problem Set 3/index.html#step-9",
    "title": "Problem Set 3",
    "section": "Step 9",
    "text": "Step 9\n\nselected_features &lt;- c(\"Mfg_Year_2001\", \"KM\")\n\ntraining_subset &lt;- training[, c(selected_features)]\n\n\ntrain_ctrl =trainControl(method = \"repeatedcv\", number =20, repeats =10)\ntree =train(Price ~ .,\n            data= training, \n            method= \"rpart\",\n            trControl = train_ctrl,\n            tuneGrid= expand.grid(cp = seq(0.0, 0.1, 0.01)))\n            control =rpart.control(method = \"anova\", minsplit =1, minibucket =1)\ntree\n\nCART \n\n1007 samples\n  87 predictor\n\nNo pre-processing\nResampling: Cross-Validated (20 fold, repeated 10 times) \nSummary of sample sizes: 956, 957, 958, 958, 957, 956, ... \nResampling results across tuning parameters:\n\n  cp    RMSE      Rsquared   MAE      \n  0.00  1212.940  0.8863993   888.3424\n  0.01  1359.279  0.8582986  1008.9520\n  0.02  1600.608  0.8010663  1211.2472\n  0.03  1612.863  0.7981088  1223.7750\n  0.04  1714.775  0.7769796  1250.9736\n  0.05  1731.201  0.7735075  1262.6367\n  0.06  1731.201  0.7735075  1262.6367\n  0.07  1731.201  0.7735075  1262.6367\n  0.08  1731.201  0.7735075  1262.6367\n  0.09  1731.201  0.7735075  1262.6367\n  0.10  1731.201  0.7735075  1262.6367\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0."
  },
  {
    "objectID": "posts/Problem Set 3/index.html#step-10",
    "href": "posts/Problem Set 3/index.html#step-10",
    "title": "Problem Set 3",
    "section": "Step 10",
    "text": "Step 10\n\ntest_predictions &lt;- predict(tree, newdata = testing)\ntest_rmse &lt;- postResample(pred = test_predictions,\n                          obs = testing$Price)\ntest_rmse\n\n        RMSE     Rsquared          MAE \n1334.5084943    0.8723882  968.1588640 \n\n\nA lower RMSE for our training data indicates that there’s less error and a more precise predictions. A larger RMSE for our testing data indicates that there’s more error and less precise predictions. Our model predicts things better with more features, but we can take out less important ones. What this means for CorollaCrowd is that it works better on training data, but it may not do the same for new data sets that come out. For example, these new datasets could be more Corolla data coming in."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Problem Set 3\n\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nJeffrey Zhao\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts/Demo Post 2/index.html#understanding-the-data",
    "href": "posts/Demo Post 2/index.html#understanding-the-data",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]