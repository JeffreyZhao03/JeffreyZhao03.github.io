/---
title: "Final Project"
author: "Annika G. Lee and Jeffrey Zhao "
date: "December 15, 2023"
output: html_document
---

## Define Business Problem

Our business problem is to develop a predictive model that accurately estimates health insurance charges or costs from individuals based on factors such as age, BMI, gender, number of children, smoking status, and region.

## Three High Quality Questions

1.  How can our model identify distinct customer     segments that are similar to each other?
2.  What strategies can we develop to offer         customers insurance that is best tailored to     their needs based on their unique characteristics?  
3.  How can we take what we learn from the model     to best implement it and see what areas we      can improve on when it comes to what we         charge? 

## Translate Business Problem into an Analytics Problem

We can attempt to develop a regression based predictive model. The objective would be to build and train a model that accurately predicts health insurance charges. One approach is to utilize a dataset that contains these factors. Our main goal would be to minimize the predictive  error.  

## How CRISP-DM would apply to the process of providing a Solution?

We could use CRISP-DM as a way to provide a solution to the process. CRISP-DM is used as a framework for data mining projects and has 6 steps. The first step is business understanding. We have to first identify what our objectives are and what constitutes  our model as being successful. The next step is data understanding. We have to explore the data set and see if there are any relationships between factors. The third step is data preparation. In this step we check for missing variables, and convert variables into factors. The fourth step is about modeling. In this step we split the data set into training and testing sets. Next we train the model. The fifth step is about evaluation. We have to evaluate how our model performed by looking at its accuracy and error. The final step is deployment. Once we are satisfied with our model, we use the model on new, unseen data. 

## Prepare Data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#| message: false
rm(list = ls())
library(tidyverse)
library(glmnet)
library(lubridate)
library(caret)
library(dummy)
library(gamlr)
library(rmarkdown)
library(GGally)
library(rpart)
library(rpart.plot)
library(corrplot)
```

## Read in Data

```{r}
insurance = read_csv("insurance.csv")
```

```{r}
glimpse(insurance)
```

## Check For Missing Variables

We can see that there are no missing variables within our data. This is a good sign for we do not have to replace any NA's within this dataset.

```{r}
sapply(insurance, function(x) mean(is.na(x)))
```

## Turn Variables into Factors

```{r}
insurance_fct = insurance %>%
  select(-age, -bmi, -charges) %>%
  mutate_all(.funs = factor)

insurance_num = insurance %>%
  select(age, bmi, charges)

insurance = bind_cols(insurance_num, insurance_fct)
```

## Summary

We can see that the minimum age is 18, because minors cant purchase health insurance. We can also see that the median age is 39 years old. One thing we notice is that the mean and median for bmi is around 30, which goes into the obesity category. This could be a big factor in health insurance costs. One of the most important parts of the data are charges. We see that the minimum health insurance is 1122, and the median health insurance charge is 9382. The max health insurance charge is 63770. This is a big discrepancy that may come into play. 


```{r}
summary(insurance)
```

## Correlation

We can see that our numeric values do not have strong correlations with one another, indicating that strong relationships are not present within our dataset. This is a good result to have for we will not have to exclude any features from our dataset.

```{r}
insurance %>%
  keep(is.numeric) %>%
  cor() %>%
  corrplot::corrplot(., method = "number", type = "lower", number.cex = 0.6, tl.cex = 0.7)
```

## Histogram

When looking at our Histogram, we seem to have a right-skewed distribution within our model. This means that the majority of the data points are on the left side, with a tail that goes to the right.

```{r}
insurance %>%
  ggplot(aes(charges)) +
  geom_histogram(color = "black", bg = "skyblue") +
  labs(title = "Distribution of Charges",
       x = "Charges",
       y = "Count of patients") +
  theme_classic()
```

## Convert Variables into Dummy Variables

```{r}
insurance_dum = dummy(insurance, int = TRUE)
insurance_num = insurance %>%
  keep(is.numeric)
insurance = bind_cols(insurance_num, insurance_dum)
rm(insurance_dum, insurance_num)
```

## Partition Data

```{r}
#Partition Data
set.seed(123)
idx = createDataPartition(insurance$charges, p = 0.7, list = FALSE)
train = insurance[idx, ]
test = insurance[-idx, ]
rm(idx)
```

## Decision Tree

```{r}
train_model = train(charges ~ .,
                   data = train,
                   method = "rpart",
                   trControl = trainControl(method = "cv", number = 10),
                   tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),
                   control = rpart.control(minbucket = 1)
                   )
plot(train_model)
```

```{r}
library(rpart.plot)
rpart.plot(train_model$finalModel)
```

## Feature Importance

When looking at our data, we can see that the most important features are: `smoker_no` being the most iportant, `bmi`, and `age`.

```{r}
library(iml)
library(patchwork)

tree_predictor = iml::Predictor$new(train_model,
                                    data = test,
                                    y = test$charges)

tree_imp = iml::FeatureImp$new(tree_predictor, loss = "rmse", compare = "ratio")
plot(tree_imp)
```

```{r}
tree_imp$results %>%
  filter(importance > 1)
```

## Train New Model

We will develop a new model using those three features we found to have the most important features. This will allow us to focus on key factors that significantly affect health insurance charges. By focusing on only the most impactful variables, we can improve the predictive performance. 

```{r}
train_new = dplyr::select(train, smoker_no, bmi, age, charges)

new_tree = caret::train(charges ~ .,
                        data = train_new,
                        method = "rpart",
                        trControl = trainControl(method = "cv", number = 10),
                        tuneGrid = expand.grid(cp = seq(0.0, 0.01, 0.0001)),
                        control = rpart.control(minbucket = 1)
                        )
plot(new_tree)
```

```{r}
rpart.plot(new_tree$finalModel)
```

## Interpret Error

When looking at our error, we see that our Testing Data has the highest RMSE.

Our Training and Cross-Validation data seem to have very similar RMSE, but our Cross- Validation is higher.

Looking at this, we can interpret that the model's performance on the testing data suggests it may have trouble if we apply it to new, unseen data. A similar RMSE between training and cross validation means that the model is fitting well to the training data. 

```{r}
train_error = postResample(predict(new_tree, train), train$charges)[["RMSE"]]

cv_error = min(new_tree$results$RMSE)

test_error = postResample(predict(new_tree, test), test$charges)[["RMSE"]]

data.frame(
  "Error Source" = c("Training", "Cross-Validation", "Testing"),
  "RMSE" = c(train_error, cv_error, test_error)
)
```

## Conclusion
In conclusion, our project sought to develop a predictive model for estimating health insurance charges based on factors such as demographics and lifestyle. We used the CRISP-DM for the machine learning process as a way to guide our project. Our findings lead us to reveal that the factors that influence health insurance charges the most are age, BMI, and smoking status. However, we found out the model's performance, on testing data, had challenges we needed to look into. We found out valuable insight into the decision making process involved in estimating healthcare pricing. 

